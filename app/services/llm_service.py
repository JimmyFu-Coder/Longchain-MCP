# app/services/llm_service.py
from langchain_openai import AzureChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.callbacks import get_openai_callback
from app.core.config import settings
from app.services.rag_service import rag_service
from app.services.azure_search_service import azure_search_service
from typing import List, Dict, Any
import tiktoken
import json

# Initialize Azure LLM client
llm = AzureChatOpenAI(
    api_key=settings.azure_openai_api_key,
    azure_endpoint=settings.azure_openai_endpoint,
    model=settings.azure_deployment_name,
    api_version=settings.azure_api_version,
    temperature=0.7,
    stream_usage=True,  # å¯ç”¨æµå¼ä½¿ç”¨ç»Ÿè®¡
)

# å…¨å±€å¯¹è¯å†å²å­˜å‚¨
conversation_history: List[HumanMessage | AIMessage] = []

# Tokenè®¡ç®—å·¥å…·å‡½æ•°
def count_tokens_for_messages(messages: List[HumanMessage | AIMessage], model: str = "gpt-4") -> int:
    """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„tokenæ•°é‡"""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("o200k_base")

    total_tokens = 0
    for message in messages:
        if message.content:  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            # æ¯æ¡æ¶ˆæ¯åŸºç¡€å¼€é”€çº¦4ä¸ªtoken
            total_tokens += 4
            total_tokens += len(encoding.encode(str(message.content)))

    # å¯¹è¯ç»“æŸæ ‡è®°
    total_tokens += 2
    return total_tokens

# ğŸ”¹ Non-streaming version with RAG support
async def process_prompt(prompt: str, use_rag: bool = True) -> Dict[str, Any]:
    # RAGå¤„ç†ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶å¢å¼ºprompt
    if use_rag:
        rag_result = await rag_service.process_query_with_rag(prompt)
        enhanced_prompt = rag_result["enhanced_prompt"]
        context_info = rag_result["context_info"]
    else:
        enhanced_prompt = prompt
        context_info = {"has_context": False}

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    user_message = HumanMessage(content=enhanced_prompt)
    conversation_history.append(user_message)

    # è®¡ç®—è¾“å…¥token
    input_tokens = count_tokens_for_messages(conversation_history)

    # ä½¿ç”¨callbackè¿½è¸ªtokenä½¿ç”¨
    with get_openai_callback() as cb:
        response = await llm.ainvoke(conversation_history)

    # æ·»åŠ AIå›å¤åˆ°å†å²
    ai_message = AIMessage(content=response.content)
    conversation_history.append(ai_message)

    # è®¡ç®—è¾“å‡ºtoken (ä»…AIå›å¤)
    output_tokens = count_tokens_for_messages([ai_message])

    return {
        "response": response.content,
        "context_info": context_info,
        "original_query": prompt,
        "enhanced_prompt": enhanced_prompt if use_rag else None,
        "token_usage": {
            "input_tokens": cb.prompt_tokens if hasattr(cb, 'prompt_tokens') else input_tokens,
            "output_tokens": cb.completion_tokens if hasattr(cb, 'completion_tokens') else output_tokens,
            "total_tokens": cb.total_tokens if hasattr(cb, 'total_tokens') else (input_tokens + output_tokens)
        }
    }

# âœ¨ Streaming version with RAG support
async def stream_prompt(prompt: str, use_rag: bool = True):
    """
    Stream LLM output chunk by chunk with RAG support.
    Used for StreamingResponse in FastAPI.
    """
    # RAGå¤„ç†ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶å¢å¼ºprompt
    if use_rag:
        rag_result = await rag_service.process_query_with_rag(prompt)
        enhanced_prompt = rag_result["enhanced_prompt"]
        context_info = rag_result["context_info"]

        # å‘é€ä¸Šä¸‹æ–‡ä¿¡æ¯ç»™å‰ç«¯ï¼ˆå¯é€‰ï¼‰
        if context_info.get("has_context", False):
            semantic_info = " (with semantic search)" if context_info.get("semantic_search_used", False) else ""
            context_notice = f"[CONTEXT_FOUND]Found {context_info.get('chunk_count', 0)} relevant document chunks{semantic_info}[/CONTEXT_FOUND]\n\n"
            yield context_notice.encode("utf-8")
    else:
        enhanced_prompt = prompt
        context_info = {"has_context": False}

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    user_message = HumanMessage(content=enhanced_prompt)
    conversation_history.append(user_message)

    # è®¡ç®—è¾“å…¥token
    input_tokens = count_tokens_for_messages(conversation_history)
    print(f"è®¡ç®—çš„è¾“å…¥token: {input_tokens}")  # è°ƒè¯•ä¿¡æ¯

    # ç”¨äºæ”¶é›†å®Œæ•´å›å¤çš„å˜é‡
    full_response = ""
    usage_info = None

    try:
        # ä½¿ç”¨å®Œæ•´å†å²è¿›è¡Œæµå¼æ¨ç†
        async for chunk in llm.astream(conversation_history):
            if chunk.content:
                full_response += chunk.content
                # NOTE: must yield bytes when using StreamingResponse
                yield chunk.content.encode("utf-8")

            # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯ï¼ˆåœ¨æµçš„æœ€åï¼‰
            if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                usage_info = chunk.usage_metadata
                print(f"ä»LLMè·å–çš„usageä¿¡æ¯: {usage_info}")  # è°ƒè¯•ä¿¡æ¯

        # æµå¼å®Œæˆåï¼Œæ·»åŠ AIå›å¤åˆ°å†å²
        if full_response:
            ai_message = AIMessage(content=full_response)
            conversation_history.append(ai_message)

            # è®¡ç®—è¾“å‡ºtoken
            output_tokens = count_tokens_for_messages([ai_message])
            print(f"è®¡ç®—çš„è¾“å‡ºtoken: {output_tokens}")  # è°ƒè¯•ä¿¡æ¯

            # å‘é€tokenç»Ÿè®¡ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
            token_stats = {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens,
                "rag_used": use_rag,
                "context_found": context_info.get("has_context", False),
                "source_chunks": len(context_info.get("sources", [])),
                "semantic_search_used": context_info.get("semantic_search_used", False),
                "azure_search_powered": True
            }
            print(f"æœ€ç»ˆtokenç»Ÿè®¡: {token_stats}")  # è°ƒè¯•ä¿¡æ¯

            # ä»¥ç‰¹æ®Šæ ¼å¼å‘é€tokenç»Ÿè®¡ï¼ˆå¯è¢«å‰ç«¯è¯†åˆ«ï¼‰
            token_info = f"\n\n[TOKEN_USAGE]{json.dumps(token_stats)}[/TOKEN_USAGE]"
            yield token_info.encode("utf-8")

    except Exception as e:
        # Optional: log the error here
        error_msg = f"[Error] {str(e)}"
        yield error_msg.encode("utf-8")

# è·å–tokenç»Ÿè®¡çš„æµå¼ç‰ˆæœ¬ï¼ˆè¿”å›å®Œæ•´å“åº”å’Œtokenä¿¡æ¯ï¼‰
async def stream_prompt_with_stats(prompt: str) -> Dict[str, Any]:
    """
    æµå¼å¤„ç†å¹¶è¿”å›å®Œæ•´å“åº”å’Œtokenç»Ÿè®¡ä¿¡æ¯
    """
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    user_message = HumanMessage(content=prompt)
    conversation_history.append(user_message)

    # è®¡ç®—è¾“å…¥token
    input_tokens = count_tokens_for_messages(conversation_history)

    # ç”¨äºæ”¶é›†å®Œæ•´å›å¤çš„å˜é‡
    full_response = ""
    usage_info = None

    try:
        # ä½¿ç”¨å®Œæ•´å†å²è¿›è¡Œæµå¼æ¨ç†
        async for chunk in llm.astream(conversation_history):
            if chunk.content:
                full_response += chunk.content

            # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯
            if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                usage_info = chunk.usage_metadata

        # æµå¼å®Œæˆåï¼Œæ·»åŠ AIå›å¤åˆ°å†å²
        ai_message = None
        if full_response:
            ai_message = AIMessage(content=full_response)
            conversation_history.append(ai_message)

        # è®¡ç®—è¾“å‡ºtoken
        output_tokens = count_tokens_for_messages([ai_message]) if ai_message else 0

        return {
            "response": full_response,
            "token_usage": {
                "input_tokens": usage_info.get('input_tokens', input_tokens) if usage_info else input_tokens,
                "output_tokens": usage_info.get('output_tokens', output_tokens) if usage_info else output_tokens,
                "total_tokens": usage_info.get('total_tokens', input_tokens + output_tokens) if usage_info else (input_tokens + output_tokens)
            }
        }

    except Exception as e:
        return {
            "response": f"[Error] {str(e)}",
            "token_usage": {
                "input_tokens": input_tokens,
                "output_tokens": 0,
                "total_tokens": input_tokens
            }
        }
