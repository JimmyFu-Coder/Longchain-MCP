# app/services/llm_service.py
from langchain_openai import AzureChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.callbacks import get_openai_callback
from app.core.config import settings
from app.services.rag_service import rag_service
from app.services.azure_search_service import azure_search_service
from app.services.mcp_service import mcp_service
from typing import List, Dict, Any
import asyncio
import tiktoken
import json

# Initialize Azure LLM client
llm = AzureChatOpenAI(
    api_key=settings.azure_openai_api_key,
    azure_endpoint=settings.azure_openai_endpoint,
    model=settings.azure_deployment_name,
    api_version=settings.azure_api_version,
    temperature=0.7,
)

# æ•°æ®åº“å·¥å…·å®šä¹‰
DATABASE_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "query_database",
            "description": "ALWAYS use this tool when user asks about database content, tables, schemas, or data. Execute SQL queries to get table information, schema details, or retrieve data from the PostgreSQL database. For schema queries, use: SELECT schema_name FROM information_schema.schemata ORDER BY schema_name;",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Any SQL SELECT query to execute. You can query system tables for metadata, get schema info, or retrieve data."
                    },
                    "params": {
                        "type": "array",
                        "description": "Optional parameters for prepared statements",
                        "items": {"type": "string"}
                    }
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "list_tables",
            "description": "ALWAYS use this tool when user asks 'what tables are in the database', 'list tables', 'show tables', or similar questions. Lists all tables in the specified database schema.",
            "parameters": {
                "type": "object",
                "properties": {
                    "schema": {
                        "type": "string",
                        "description": "Schema name to list tables from (default: public)",
                        "default": "public"
                    }
                }
            }
        }
    }
]

# å…¨å±€å¯¹è¯å†å²å­˜å‚¨
conversation_history: List[HumanMessage | AIMessage] = []

# Tokenè®¡ç®—å·¥å…·å‡½æ•°
def count_tokens_for_messages(messages: List[HumanMessage | AIMessage], model: str = "gpt-4") -> int:
    """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„tokenæ•°é‡"""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("o200k_base")

    total_tokens = 0
    for message in messages:
        if message.content:  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            # æ¯æ¡æ¶ˆæ¯åŸºç¡€å¼€é”€çº¦4ä¸ªtoken
            total_tokens += 4
            total_tokens += len(encoding.encode(str(message.content)))

    # å¯¹è¯ç»“æŸæ ‡è®°
    total_tokens += 2
    return total_tokens

# å¤„ç†å‡½æ•°è°ƒç”¨
async def handle_function_call(function_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
    """å¤„ç†LLMçš„å‡½æ•°è°ƒç”¨"""
    try:
        # ç¡®ä¿MCPæœåŠ¡å·²å¯åŠ¨
        if not mcp_service.process:
            startup_success = await asyncio.wait_for(mcp_service.start(), timeout=5.0)
            if not startup_success:
                return {"success": False, "error": "MCP service failed to start"}

        # è°ƒç”¨MCPæœåŠ¡
        result = await asyncio.wait_for(mcp_service.call_tool(function_name, arguments), timeout=10.0)
        return result
    except Exception as e:
        return {
            "success": False,
            "error": f"Function call failed: {str(e)}"
        }

# ğŸ”¹ Non-streaming version with RAG support
async def process_prompt(prompt: str, use_rag: bool = True, use_tools: bool = True) -> Dict[str, Any]:
    # RAGå¤„ç†ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶å¢å¼ºprompt
    if use_rag:
        rag_result = await rag_service.process_query_with_rag(prompt)
        enhanced_prompt = rag_result["enhanced_prompt"]
        context_info = rag_result["context_info"]
    else:
        enhanced_prompt = prompt
        context_info = {"has_context": False}

    # æ·»åŠ åŸå§‹ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²ï¼ˆç”¨äºè®°å¿†ï¼‰
    user_message = HumanMessage(content=prompt)
    conversation_history.append(user_message)

    # æ„å»ºæ¨ç†ä¸Šä¸‹æ–‡ï¼ˆä¸´æ—¶çš„ï¼ŒåŒ…å«RAGå¢å¼ºï¼‰
    if use_rag and enhanced_prompt != prompt:
        # ç”¨å¢å¼ºç‰ˆæœ¬æ›¿æ¢æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯è¿›è¡Œæ¨ç†
        inference_context = conversation_history[:-1] + [HumanMessage(content=enhanced_prompt)]
    else:
        inference_context = conversation_history

    # è®¡ç®—è¾“å…¥token
    input_tokens = count_tokens_for_messages(inference_context)

    # ä½¿ç”¨callbackè¿½è¸ªtokenä½¿ç”¨ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨
    with get_openai_callback() as cb:
        if use_tools:
            response = await llm.ainvoke(inference_context, tools=DATABASE_TOOLS)
        else:
            response = await llm.ainvoke(inference_context)

    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
    if hasattr(response, 'tool_calls') and response.tool_calls:
        tool_results = []
        for tool_call in response.tool_calls:

            # æ ¹æ®å®é™…æ ¼å¼è§£æ
            if isinstance(tool_call, dict) and 'name' in tool_call:
                function_name = tool_call['name']
                arguments = tool_call.get('args', {})
            elif hasattr(tool_call, 'function'):
                function_name = tool_call.function.name
                arguments = json.loads(tool_call.function.arguments)
            elif isinstance(tool_call, dict) and 'function' in tool_call:
                function_name = tool_call['function']['name']
                arguments = json.loads(tool_call['function']['arguments'])
            else:
                print(f"Unknown tool_call format: {tool_call}")
                continue

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_result = await handle_function_call(function_name, arguments)
            tool_results.append({
                "function_name": function_name,
                "arguments": arguments,
                "result": tool_result
            })

            # å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
            tool_message = AIMessage(content=f"è°ƒç”¨äº†å‡½æ•° {function_name}ï¼Œç»“æœï¼š{json.dumps(tool_result, ensure_ascii=False)}")
            conversation_history.append(tool_message)

        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œé‡æ–°è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›å¤
        final_response = await llm.ainvoke(conversation_history)
        ai_message = AIMessage(content=final_response.content)
        conversation_history.append(ai_message)

        return {
            "response": final_response.content,
            "context_info": context_info,
            "original_query": prompt,
            "enhanced_prompt": enhanced_prompt if use_rag else None,
            "tool_calls": tool_results,
            "token_usage": {
                "input_tokens": cb.prompt_tokens if hasattr(cb, 'prompt_tokens') else input_tokens,
                "output_tokens": cb.completion_tokens if hasattr(cb, 'completion_tokens') else output_tokens,
                "total_tokens": cb.total_tokens if hasattr(cb, 'total_tokens') else (input_tokens + output_tokens)
            }
        }

    # æ·»åŠ AIå›å¤åˆ°å†å²
    ai_message = AIMessage(content=response.content)
    conversation_history.append(ai_message)

    # è®¡ç®—è¾“å‡ºtoken (ä»…AIå›å¤)
    output_tokens = count_tokens_for_messages([ai_message])

    return {
        "response": response.content,
        "context_info": context_info,
        "original_query": prompt,
        "enhanced_prompt": enhanced_prompt if use_rag else None,
        "token_usage": {
            "input_tokens": cb.prompt_tokens if hasattr(cb, 'prompt_tokens') else input_tokens,
            "output_tokens": cb.completion_tokens if hasattr(cb, 'completion_tokens') else output_tokens,
            "total_tokens": cb.total_tokens if hasattr(cb, 'total_tokens') else (input_tokens + output_tokens)
        }
    }

# âœ¨ Streaming version with RAG support
async def stream_prompt(prompt: str, use_rag: bool = True, use_tools: bool = True):
    """
    Stream LLM output chunk by chunk with RAG support.
    Used for StreamingResponse in FastAPI.
    """
    # RAGå¤„ç†ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶å¢å¼ºprompt
    if use_rag:
        rag_result = await rag_service.process_query_with_rag(prompt)
        enhanced_prompt = rag_result["enhanced_prompt"]
        context_info = rag_result["context_info"]

        # å‘é€ä¸Šä¸‹æ–‡ä¿¡æ¯ç»™å‰ç«¯ï¼ˆå¯é€‰ï¼‰
        if context_info.get("has_context", False):
            semantic_info = " (with semantic search)" if context_info.get("semantic_search_used", False) else ""
            context_notice = f"[CONTEXT_FOUND]Found {context_info.get('chunk_count', 0)} relevant document chunks{semantic_info}[/CONTEXT_FOUND]\n\n"
            yield context_notice.encode("utf-8")
    else:
        enhanced_prompt = prompt
        context_info = {"has_context": False}

    # æ·»åŠ åŸå§‹ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²ï¼ˆç”¨äºè®°å¿†ï¼‰
    user_message = HumanMessage(content=prompt)
    conversation_history.append(user_message)

    # æ„å»ºæ¨ç†ä¸Šä¸‹æ–‡ï¼ˆä¸´æ—¶çš„ï¼ŒåŒ…å«RAGå¢å¼ºï¼‰
    if use_rag and enhanced_prompt != prompt:
        # ç”¨å¢å¼ºç‰ˆæœ¬æ›¿æ¢æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯è¿›è¡Œæ¨ç†
        inference_context = conversation_history[:-1] + [HumanMessage(content=enhanced_prompt)]
    else:
        inference_context = conversation_history

    # è®¡ç®—è¾“å…¥token
    input_tokens = count_tokens_for_messages(inference_context)
    print(f"è®¡ç®—çš„è¾“å…¥token: {input_tokens}")  # è°ƒè¯•ä¿¡æ¯
    print("å¼€å§‹LLMæµå¼è°ƒç”¨...")  # è°ƒè¯•ä¿¡æ¯

    # ç”¨äºæ”¶é›†å®Œæ•´å›å¤çš„å˜é‡
    full_response = ""
    usage_info = None

    try:
        # ä½¿ç”¨æ¨ç†ä¸Šä¸‹æ–‡è¿›è¡Œæµå¼æ¨ç†
        if use_tools:
            tool_calls_collected = {}  # æ”¶é›†å®Œæ•´çš„å·¥å…·è°ƒç”¨

            async for chunk in llm.astream(inference_context, tools=DATABASE_TOOLS):
                print(f"æ”¶åˆ°chunk: {chunk}")  # è°ƒè¯•ä¿¡æ¯

                # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
                if hasattr(chunk, 'tool_call_chunks') and chunk.tool_call_chunks:
                    for tool_chunk in chunk.tool_call_chunks:
                        call_id = tool_chunk.get('id')
                        if call_id and call_id not in tool_calls_collected:
                            tool_calls_collected[call_id] = {
                                'name': tool_chunk.get('name', ''),
                                'args': tool_chunk.get('args', ''),
                                'id': call_id
                            }
                        elif call_id and call_id in tool_calls_collected:
                            # ç»§ç»­ç´¯ç§¯å‚æ•°
                            tool_calls_collected[call_id]['args'] += tool_chunk.get('args', '')
                        elif not call_id:
                            # å¤„ç†æ²¡æœ‰call_idçš„å‚æ•°ç‰‡æ®µï¼ˆAzure OpenAIçš„åˆ†ç‰‡ä¼ è¾“ï¼‰
                            # è¿™äº›åº”è¯¥å±äºæœ€è¿‘çš„å·¥å…·è°ƒç”¨
                            if tool_calls_collected:
                                latest_call_id = list(tool_calls_collected.keys())[-1]
                                tool_calls_collected[latest_call_id]['args'] += tool_chunk.get('args', '')

                # æ£€æŸ¥æ˜¯å¦å®Œæˆäº†å·¥å…·è°ƒç”¨ï¼ˆæ”¶åˆ°finish_reasonï¼‰
                if hasattr(chunk, 'response_metadata') and chunk.response_metadata.get('finish_reason') == 'tool_calls':
                    print(f"å·¥å…·è°ƒç”¨å®Œæˆï¼Œå¤„ç†æ”¶é›†åˆ°çš„å·¥å…·è°ƒç”¨: {tool_calls_collected}")  # è°ƒè¯•ä¿¡æ¯
                    yield "[TOOL_CALLS_COMPLETE]".encode("utf-8")

                    # å¤„ç†æ‰€æœ‰æ”¶é›†åˆ°çš„å·¥å…·è°ƒç”¨
                    tool_results = []
                    for call_id, tool_info in tool_calls_collected.items():
                        if tool_info['name']:
                            try:
                                arguments = json.loads(tool_info['args']) if tool_info['args'] else {}
                            except json.JSONDecodeError:
                                arguments = {}

                            print(f"è°ƒç”¨å·¥å…·: {tool_info['name']} with args: {arguments}")  # è°ƒè¯•ä¿¡æ¯
                            tool_result = await handle_function_call(tool_info['name'], arguments)
                            print(f"å·¥å…·è°ƒç”¨ç»“æœ: {tool_result}")  # è°ƒè¯•ä¿¡æ¯

                            tool_results.append({
                                "function_name": tool_info['name'],
                                "arguments": arguments,
                                "result": tool_result
                            })

                            # å‘é€å·¥å…·è°ƒç”¨ç»“æœç»™å‰ç«¯
                            result_msg = f"[TOOL_RESULT]{json.dumps(tool_result, ensure_ascii=False)}[/TOOL_RESULT]"
                            yield result_msg.encode("utf-8")

                    # å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
                    for tool_result in tool_results:
                        tool_message = AIMessage(content=f"è°ƒç”¨äº†å‡½æ•° {tool_result['function_name']}ï¼Œç»“æœï¼š{json.dumps(tool_result['result'], ensure_ascii=False)}")
                        conversation_history.append(tool_message)

                    # é‡æ–°è°ƒç”¨LLMç”ŸæˆåŸºäºå·¥å…·ç»“æœçš„æœ€ç»ˆå›å¤
                    print("é‡æ–°è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›å¤...")  # è°ƒè¯•ä¿¡æ¯
                    yield "\n\n[GENERATING_FINAL_RESPONSE]".encode("utf-8")

                    async for final_chunk in llm.astream(conversation_history):
                        if final_chunk.content:
                            full_response += final_chunk.content
                            yield final_chunk.content.encode("utf-8")

                    # è·³å‡ºä¸»å¾ªç¯ï¼Œå› ä¸ºå·²ç»å®Œæˆäº†å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹
                    break

                if chunk.content:
                    full_response += chunk.content
                    # NOTE: must yield bytes when using StreamingResponse
                    yield chunk.content.encode("utf-8")

                # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯ï¼ˆåœ¨æµçš„æœ€åï¼‰
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    usage_info = chunk.usage_metadata
                    print(f"ä»LLMè·å–çš„usageä¿¡æ¯: {usage_info}")  # è°ƒè¯•ä¿¡æ¯
        else:
            async for chunk in llm.astream(inference_context):
                if chunk.content:
                    full_response += chunk.content
                    # NOTE: must yield bytes when using StreamingResponse
                    yield chunk.content.encode("utf-8")

                # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯ï¼ˆåœ¨æµçš„æœ€åï¼‰
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    usage_info = chunk.usage_metadata
                    print(f"ä»LLMè·å–çš„usageä¿¡æ¯: {usage_info}")  # è°ƒè¯•ä¿¡æ¯

        # æµå¼å®Œæˆåï¼Œæ·»åŠ AIå›å¤åˆ°å†å²
        if full_response:
            ai_message = AIMessage(content=full_response)
            conversation_history.append(ai_message)

            # è®¡ç®—è¾“å‡ºtoken
            output_tokens = count_tokens_for_messages([ai_message])
            print(f"è®¡ç®—çš„è¾“å‡ºtoken: {output_tokens}")  # è°ƒè¯•ä¿¡æ¯

            # å‘é€tokenç»Ÿè®¡ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
            token_stats = {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens,
                "rag_used": use_rag,
                "context_found": context_info.get("has_context", False),
                "source_chunks": len(context_info.get("sources", [])),
                "semantic_search_used": context_info.get("semantic_search_used", False),
                "azure_search_powered": True
            }
            print(f"æœ€ç»ˆtokenç»Ÿè®¡: {token_stats}")  # è°ƒè¯•ä¿¡æ¯

            # ä»¥ç‰¹æ®Šæ ¼å¼å‘é€tokenç»Ÿè®¡ï¼ˆå¯è¢«å‰ç«¯è¯†åˆ«ï¼‰
            token_info = f"\n\n[TOKEN_USAGE]{json.dumps(token_stats)}[/TOKEN_USAGE]"
            yield token_info.encode("utf-8")

    except Exception as e:
        # Optional: log the error here
        error_msg = f"[Error] {str(e)}"
        yield error_msg.encode("utf-8")

# è·å–tokenç»Ÿè®¡çš„æµå¼ç‰ˆæœ¬ï¼ˆè¿”å›å®Œæ•´å“åº”å’Œtokenä¿¡æ¯ï¼‰
async def stream_prompt_with_stats(prompt: str) -> Dict[str, Any]:
    """
    æµå¼å¤„ç†å¹¶è¿”å›å®Œæ•´å“åº”å’Œtokenç»Ÿè®¡ä¿¡æ¯
    """
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    user_message = HumanMessage(content=prompt)
    conversation_history.append(user_message)

    # è®¡ç®—è¾“å…¥token
    input_tokens = count_tokens_for_messages(conversation_history)

    # ç”¨äºæ”¶é›†å®Œæ•´å›å¤çš„å˜é‡
    full_response = ""
    usage_info = None

    try:
        # ä½¿ç”¨å®Œæ•´å†å²è¿›è¡Œæµå¼æ¨ç†
        async for chunk in llm.astream(conversation_history):
            if chunk.content:
                full_response += chunk.content

            # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯
            if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                usage_info = chunk.usage_metadata

        # æµå¼å®Œæˆåï¼Œæ·»åŠ AIå›å¤åˆ°å†å²
        ai_message = None
        if full_response:
            ai_message = AIMessage(content=full_response)
            conversation_history.append(ai_message)

        # è®¡ç®—è¾“å‡ºtoken
        output_tokens = count_tokens_for_messages([ai_message]) if ai_message else 0

        return {
            "response": full_response,
            "token_usage": {
                "input_tokens": usage_info.get('input_tokens', input_tokens) if usage_info else input_tokens,
                "output_tokens": usage_info.get('output_tokens', output_tokens) if usage_info else output_tokens,
                "total_tokens": usage_info.get('total_tokens', input_tokens + output_tokens) if usage_info else (input_tokens + output_tokens)
            }
        }

    except Exception as e:
        return {
            "response": f"[Error] {str(e)}",
            "token_usage": {
                "input_tokens": input_tokens,
                "output_tokens": 0,
                "total_tokens": input_tokens
            }
        }
